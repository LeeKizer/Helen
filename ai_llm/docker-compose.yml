version: "3.8"

services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:3000"
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
    volumes:
      - openwebui_data:/app/backend/data
    depends_on:
      - ollama
    restart: unless-stopped

  textgen-webui:
    image: ghcr.io/oobabooga/text-generation-webui:latest
    container_name: textgen-webui
    ports:
      - "7860:7860"
    volumes:
      - textgen_data:/app
    restart: unless-stopped

  jupyter:
    image: jupyter/base-notebook
    container_name: jupyter
    ports:
      - "8888:8888"
    volumes:
      - jupyter_data:/home/jovyan/work
    environment:
      - JUPYTER_TOKEN=changeme
    restart: unless-stopped

  code-server:
    image: codercom/code-server
    container_name: code-server
    ports:
      - "8443:8443"
    volumes:
      - code_data:/home/coder/project
    environment:
      - PASSWORD=changeme
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"  # gRPC
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  langchain-server:
    image: tiangolo/uvicorn-gunicorn-fastapi:python3.11
    container_name: langchain-server
    ports:
      - "8001:80"
    volumes:
      - ./langchain_app:/app
    environment:
      - MODULE_NAME=main
    restart: unless-stopped

  llm-api:
    image: tiangolo/uvicorn-gunicorn-fastapi:python3.11
    container_name: llm-api
    ports:
      - "8002:80"
    volumes:
      - ./llm_api:/app
    environment:
      - MODULE_NAME=main
    restart: unless-stopped

volumes:
  ollama_data:
  openwebui_data:
  textgen_data:
  jupyter_data:
  code_data:
  qdrant_data:
